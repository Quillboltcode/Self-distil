digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140607662524368 [label="
 ()" fillcolor=darkolivegreen1]
	140607662499552 [label=MeanBackward0]
	140607662499408 -> 140607662499552
	140607662499408 [label=AddmmBackward0]
	140607662499264 -> 140607662499408
	140607789151824 [label="fc.bias
 (1000)" fillcolor=lightblue]
	140607789151824 -> 140607662499264
	140607662499264 [label=AccumulateGrad]
	140607662499456 -> 140607662499408
	140607662499456 [label=ViewBackward0]
	140607662499216 -> 140607662499456
	140607662499216 [label=MeanBackward1]
	140607662498880 -> 140607662499216
	140607662498880 [label=ReluBackward0]
	140607662498784 -> 140607662498880
	140607662498784 [label=AddBackward0]
	140607662498688 -> 140607662498784
	140607662498688 [label=NativeBatchNormBackward0]
	140607662498544 -> 140607662498688
	140607662498544 [label=ConvolutionBackward0]
	140607662498256 -> 140607662498544
	140607662498256 [label=ReluBackward0]
	140607662498016 -> 140607662498256
	140607662498016 [label=NativeBatchNormBackward0]
	140607662500032 -> 140607662498016
	140607662500032 [label=ConvolutionBackward0]
	140607662498736 -> 140607662500032
	140607662498736 [label=ReluBackward0]
	140607662500320 -> 140607662498736
	140607662500320 [label=AddBackward0]
	140607662500416 -> 140607662500320
	140607662500416 [label=NativeBatchNormBackward0]
	140607662500560 -> 140607662500416
	140607662500560 [label=ConvolutionBackward0]
	140607662500752 -> 140607662500560
	140607662500752 [label=ReluBackward0]
	140607662500896 -> 140607662500752
	140607662500896 [label=NativeBatchNormBackward0]
	140607662500992 -> 140607662500896
	140607662500992 [label=ConvolutionBackward0]
	140607662501184 -> 140607662500992
	140607662501184 [label=ReluBackward0]
	140607662501328 -> 140607662501184
	140607662501328 [label=AddBackward0]
	140607662501424 -> 140607662501328
	140607662501424 [label=NativeBatchNormBackward0]
	140607662501568 -> 140607662501424
	140607662501568 [label=ConvolutionBackward0]
	140607662501760 -> 140607662501568
	140607662501760 [label=ReluBackward0]
	140607662501904 -> 140607662501760
	140607662501904 [label=NativeBatchNormBackward0]
	140607662502000 -> 140607662501904
	140607662502000 [label=ConvolutionBackward0]
	140607662501376 -> 140607662502000
	140607662501376 [label=ReluBackward0]
	140607662502288 -> 140607662501376
	140607662502288 [label=AddBackward0]
	140607662502384 -> 140607662502288
	140607662502384 [label=NativeBatchNormBackward0]
	140607662502528 -> 140607662502384
	140607662502528 [label=ConvolutionBackward0]
	140607662502720 -> 140607662502528
	140607662502720 [label=ReluBackward0]
	140607662502864 -> 140607662502720
	140607662502864 [label=NativeBatchNormBackward0]
	140607662502960 -> 140607662502864
	140607662502960 [label=ConvolutionBackward0]
	140607662503152 -> 140607662502960
	140607662503152 [label=ReluBackward0]
	140607662503296 -> 140607662503152
	140607662503296 [label=AddBackward0]
	140607662503392 -> 140607662503296
	140607662503392 [label=NativeBatchNormBackward0]
	140607662503536 -> 140607662503392
	140607662503536 [label=ConvolutionBackward0]
	140607662503728 -> 140607662503536
	140607662503728 [label=ReluBackward0]
	140607662503872 -> 140607662503728
	140607662503872 [label=NativeBatchNormBackward0]
	140607662503968 -> 140607662503872
	140607662503968 [label=ConvolutionBackward0]
	140607662503344 -> 140607662503968
	140607662503344 [label=ReluBackward0]
	140607662504256 -> 140607662503344
	140607662504256 [label=AddBackward0]
	140607662504352 -> 140607662504256
	140607662504352 [label=NativeBatchNormBackward0]
	140607662504496 -> 140607662504352
	140607662504496 [label=ConvolutionBackward0]
	140607662504688 -> 140607662504496
	140607662504688 [label=ReluBackward0]
	140607662504832 -> 140607662504688
	140607662504832 [label=NativeBatchNormBackward0]
	140607662504928 -> 140607662504832
	140607662504928 [label=ConvolutionBackward0]
	140607662505120 -> 140607662504928
	140607662505120 [label=ReluBackward0]
	140607662505264 -> 140607662505120
	140607662505264 [label=AddBackward0]
	140607662505360 -> 140607662505264
	140607662505360 [label=NativeBatchNormBackward0]
	140607662505504 -> 140607662505360
	140607662505504 [label=ConvolutionBackward0]
	140607662505696 -> 140607662505504
	140607662505696 [label=ReluBackward0]
	140607662505840 -> 140607662505696
	140607662505840 [label=NativeBatchNormBackward0]
	140607662505936 -> 140607662505840
	140607662505936 [label=ConvolutionBackward0]
	140607662505312 -> 140607662505936
	140607662505312 [label=ReluBackward0]
	140607660523824 -> 140607662505312
	140607660523824 [label=AddBackward0]
	140607660523920 -> 140607660523824
	140607660523920 [label=NativeBatchNormBackward0]
	140607660524064 -> 140607660523920
	140607660524064 [label=ConvolutionBackward0]
	140607660524256 -> 140607660524064
	140607660524256 [label=ReluBackward0]
	140607660524400 -> 140607660524256
	140607660524400 [label=NativeBatchNormBackward0]
	140607660524496 -> 140607660524400
	140607660524496 [label=ConvolutionBackward0]
	140607660523872 -> 140607660524496
	140607660523872 [label=MaxPool2DWithIndicesBackward0]
	140607660524784 -> 140607660523872
	140607660524784 [label=ReluBackward0]
	140607660524880 -> 140607660524784
	140607660524880 [label=NativeBatchNormBackward0]
	140607660524976 -> 140607660524880
	140607660524976 [label=ConvolutionBackward0]
	140607660525168 -> 140607660524976
	140607789140112 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140607789140112 -> 140607660525168
	140607660525168 [label=AccumulateGrad]
	140607660524928 -> 140607660524880
	140607789140304 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140607789140304 -> 140607660524928
	140607660524928 [label=AccumulateGrad]
	140607660524592 -> 140607660524880
	140607789140400 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140607789140400 -> 140607660524592
	140607660524592 [label=AccumulateGrad]
	140607660524688 -> 140607660524496
	140607789140784 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140607789140784 -> 140607660524688
	140607660524688 [label=AccumulateGrad]
	140607660524448 -> 140607660524400
	140607789140880 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140607789140880 -> 140607660524448
	140607660524448 [label=AccumulateGrad]
	140607660524304 -> 140607660524400
	140607789140976 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140607789140976 -> 140607660524304
	140607660524304 [label=AccumulateGrad]
	140607660524208 -> 140607660524064
	140607789141360 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140607789141360 -> 140607660524208
	140607660524208 [label=AccumulateGrad]
	140607660524016 -> 140607660523920
	140607789141456 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140607789141456 -> 140607660524016
	140607660524016 [label=AccumulateGrad]
	140607660523968 -> 140607660523920
	140607789141552 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140607789141552 -> 140607660523968
	140607660523968 [label=AccumulateGrad]
	140607660523872 -> 140607660523824
	140607660523728 -> 140607662505936
	140607789141936 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140607789141936 -> 140607660523728
	140607660523728 [label=AccumulateGrad]
	140607662505888 -> 140607662505840
	140607789142032 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140607789142032 -> 140607662505888
	140607662505888 [label=AccumulateGrad]
	140607662505744 -> 140607662505840
	140607789142128 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140607789142128 -> 140607662505744
	140607662505744 [label=AccumulateGrad]
	140607662505648 -> 140607662505504
	140607789142512 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140607789142512 -> 140607662505648
	140607662505648 [label=AccumulateGrad]
	140607662505456 -> 140607662505360
	140607789142608 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140607789142608 -> 140607662505456
	140607662505456 [label=AccumulateGrad]
	140607662505408 -> 140607662505360
	140607789142704 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140607789142704 -> 140607662505408
	140607662505408 [label=AccumulateGrad]
	140607662505312 -> 140607662505264
	140607662505072 -> 140607662504928
	140607789143664 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140607789143664 -> 140607662505072
	140607662505072 [label=AccumulateGrad]
	140607662504880 -> 140607662504832
	140607789143760 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140607789143760 -> 140607662504880
	140607662504880 [label=AccumulateGrad]
	140607662504736 -> 140607662504832
	140607789143856 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140607789143856 -> 140607662504736
	140607662504736 [label=AccumulateGrad]
	140607662504640 -> 140607662504496
	140607789144240 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140607789144240 -> 140607662504640
	140607662504640 [label=AccumulateGrad]
	140607662504448 -> 140607662504352
	140607789144336 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140607789144336 -> 140607662504448
	140607662504448 [label=AccumulateGrad]
	140607662504400 -> 140607662504352
	140607789144432 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140607789144432 -> 140607662504400
	140607662504400 [label=AccumulateGrad]
	140607662504304 -> 140607662504256
	140607662504304 [label=NativeBatchNormBackward0]
	140607662505024 -> 140607662504304
	140607662505024 [label=ConvolutionBackward0]
	140607662505120 -> 140607662505024
	140607662505168 -> 140607662505024
	140607789143088 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140607789143088 -> 140607662505168
	140607662505168 [label=AccumulateGrad]
	140607662504592 -> 140607662504304
	140607789143184 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140607789143184 -> 140607662504592
	140607662504592 [label=AccumulateGrad]
	140607662504544 -> 140607662504304
	140607789143280 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140607789143280 -> 140607662504544
	140607662504544 [label=AccumulateGrad]
	140607662504160 -> 140607662503968
	140607789144816 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140607789144816 -> 140607662504160
	140607662504160 [label=AccumulateGrad]
	140607662503920 -> 140607662503872
	140607789144912 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140607789144912 -> 140607662503920
	140607662503920 [label=AccumulateGrad]
	140607662503776 -> 140607662503872
	140607789145008 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140607789145008 -> 140607662503776
	140607662503776 [label=AccumulateGrad]
	140607662503680 -> 140607662503536
	140607789145392 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140607789145392 -> 140607662503680
	140607662503680 [label=AccumulateGrad]
	140607662503488 -> 140607662503392
	140607789145488 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140607789145488 -> 140607662503488
	140607662503488 [label=AccumulateGrad]
	140607662503440 -> 140607662503392
	140607789145584 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140607789145584 -> 140607662503440
	140607662503440 [label=AccumulateGrad]
	140607662503344 -> 140607662503296
	140607662503104 -> 140607662502960
	140607789146544 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140607789146544 -> 140607662503104
	140607662503104 [label=AccumulateGrad]
	140607662502912 -> 140607662502864
	140607789146640 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140607789146640 -> 140607662502912
	140607662502912 [label=AccumulateGrad]
	140607662502768 -> 140607662502864
	140607789146736 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140607789146736 -> 140607662502768
	140607662502768 [label=AccumulateGrad]
	140607662502672 -> 140607662502528
	140607789147120 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140607789147120 -> 140607662502672
	140607662502672 [label=AccumulateGrad]
	140607662502480 -> 140607662502384
	140607789147216 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140607789147216 -> 140607662502480
	140607662502480 [label=AccumulateGrad]
	140607662502432 -> 140607662502384
	140607789147312 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140607789147312 -> 140607662502432
	140607662502432 [label=AccumulateGrad]
	140607662502336 -> 140607662502288
	140607662502336 [label=NativeBatchNormBackward0]
	140607662503056 -> 140607662502336
	140607662503056 [label=ConvolutionBackward0]
	140607662503152 -> 140607662503056
	140607662503200 -> 140607662503056
	140607789145968 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140607789145968 -> 140607662503200
	140607662503200 [label=AccumulateGrad]
	140607662502624 -> 140607662502336
	140607789146064 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140607789146064 -> 140607662502624
	140607662502624 [label=AccumulateGrad]
	140607662502576 -> 140607662502336
	140607789146160 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140607789146160 -> 140607662502576
	140607662502576 [label=AccumulateGrad]
	140607662502192 -> 140607662502000
	140607789147696 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140607789147696 -> 140607662502192
	140607662502192 [label=AccumulateGrad]
	140607662501952 -> 140607662501904
	140607789147792 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140607789147792 -> 140607662501952
	140607662501952 [label=AccumulateGrad]
	140607662501808 -> 140607662501904
	140607789147888 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140607789147888 -> 140607662501808
	140607662501808 [label=AccumulateGrad]
	140607662501712 -> 140607662501568
	140607789148272 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140607789148272 -> 140607662501712
	140607662501712 [label=AccumulateGrad]
	140607662501520 -> 140607662501424
	140607789148368 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140607789148368 -> 140607662501520
	140607662501520 [label=AccumulateGrad]
	140607662501472 -> 140607662501424
	140607789148464 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140607789148464 -> 140607662501472
	140607662501472 [label=AccumulateGrad]
	140607662501376 -> 140607662501328
	140607662501136 -> 140607662500992
	140607789149424 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140607789149424 -> 140607662501136
	140607662501136 [label=AccumulateGrad]
	140607662500944 -> 140607662500896
	140607789149520 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140607789149520 -> 140607662500944
	140607662500944 [label=AccumulateGrad]
	140607662500800 -> 140607662500896
	140607789149616 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140607789149616 -> 140607662500800
	140607662500800 [label=AccumulateGrad]
	140607662500704 -> 140607662500560
	140607789150000 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140607789150000 -> 140607662500704
	140607662500704 [label=AccumulateGrad]
	140607662500512 -> 140607662500416
	140607789150096 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140607789150096 -> 140607662500512
	140607662500512 [label=AccumulateGrad]
	140607662500464 -> 140607662500416
	140607789150192 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140607789150192 -> 140607662500464
	140607662500464 [label=AccumulateGrad]
	140607662500368 -> 140607662500320
	140607662500368 [label=NativeBatchNormBackward0]
	140607662501088 -> 140607662500368
	140607662501088 [label=ConvolutionBackward0]
	140607662501184 -> 140607662501088
	140607662501232 -> 140607662501088
	140607789148848 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140607789148848 -> 140607662501232
	140607662501232 [label=AccumulateGrad]
	140607662500656 -> 140607662500368
	140607789148944 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140607789148944 -> 140607662500656
	140607662500656 [label=AccumulateGrad]
	140607662500608 -> 140607662500368
	140607789149040 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140607789149040 -> 140607662500608
	140607662500608 [label=AccumulateGrad]
	140607662500224 -> 140607662500032
	140607789150576 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140607789150576 -> 140607662500224
	140607662500224 [label=AccumulateGrad]
	140607662497968 -> 140607662498016
	140607789150672 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140607789150672 -> 140607662497968
	140607662497968 [label=AccumulateGrad]
	140607662498160 -> 140607662498016
	140607789150768 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140607789150768 -> 140607662498160
	140607662498160 [label=AccumulateGrad]
	140607662498304 -> 140607662498544
	140607789151152 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140607789151152 -> 140607662498304
	140607662498304 [label=AccumulateGrad]
	140607662498592 -> 140607662498688
	140607789151248 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140607789151248 -> 140607662498592
	140607662498592 [label=AccumulateGrad]
	140607662498640 -> 140607662498688
	140607789151344 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140607789151344 -> 140607662498640
	140607662498640 [label=AccumulateGrad]
	140607662498736 -> 140607662498784
	140607662499312 -> 140607662499408
	140607662499312 [label=TBackward0]
	140607662498832 -> 140607662499312
	140607789151728 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	140607789151728 -> 140607662498832
	140607662498832 [label=AccumulateGrad]
	140607662499552 -> 140607662524368
}
